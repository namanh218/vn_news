import requestsfrom bs4 import BeautifulSoupfrom pymongo import MongoClientimport timefrom multiprocessing import Process# connect to atlas mongodbclient = MongoClient("mongodb+srv://m001-student:namanh123@sandbox-fwkp4.mongodb.net/test?retryWrites=true&w=majority")db = client.newscollection = db.news_collection# print all available dbprint(client.list_database_names())BASE_URL = 'https://vnexpress.net/giao-duc'# Get the linkdef parse_url(url):    try:        while True:            response = requests.get(url)            if response:                soup = BeautifulSoup(response.text, 'html.parser')                return soup            time.sleep(10)    except Exception as err:        print(err)        return ''# Get all the content and titlesdef get_content(url):    s = parse_url(url)    b = s.find_all('article', class_="item-news item-news-common off-thumb")    res = [i.get('href') for i in s.find_all('a', class_="btn-page next-page ")]    print(res)    titles = [i.find_all('a')[0].string.replace('.', '') for i in b]    des = [i.find_all('a')[1].string for i in b]    a = list(map(lambda x: {x[0]: x[1]}, zip(titles, des)))    return a, res# Save to dbdef save_todb(d):    return collection.insert_many(d)# Get all pagesdef get_all(url):    _, l = get_content(url)    while len(l) > 0:        s, l = get_content(url)        url = 'http://vnexpress.net' + ''.join(l)        if len(s) == 0:            continue        save_todb(s)    returnif __name__ == "__main__":    start = time.process_time()    p = Process(target=get_all, args=(BASE_URL,))    p.start()    p.join()    print(time.process_time() - start)